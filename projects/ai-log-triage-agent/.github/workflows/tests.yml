name: Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-dev.txt

      - name: Run tests with structured output
        run: |
          python run_tests.py --format json --output test-results-${{ matrix.os }}-py${{ matrix.python-version }}.json

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: test-results-*.json

      - name: Run tests with pytest
        run: |
          pytest tests/ --json-report=pytest-results-${{ matrix.os }}-py${{ matrix.python-version }}.json --cov=src/ai_log_triage --cov-report=xml --cov-report=html

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

      - name: Upload pytest results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: pytest-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: pytest-results-*.json

      - name: Upload coverage HTML
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-html-${{ matrix.os }}-py${{ matrix.python-version }}
          path: htmlcov/

  benchmark:
    runs-on: ubuntu-latest
    needs: test

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-dev.txt

      - name: Run performance tests
        run: |
          python run_tests.py --format json --output benchmark-results.json --verbosity 0

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json

  report:
    runs-on: ubuntu-latest
    needs: test
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          merge-multiple: true

      - name: Generate markdown report
        run: |
          python -c "
          import json
          import glob
          from pathlib import Path

          # Combine all test results
          all_results = []
          for file in glob.glob('test-results-*.json'):
              with open(file) as f:
                  data = json.load(f)
                  all_results.append({'file': file, 'data': data})

          # Generate markdown report
          with open('test-report.md', 'w') as f:
              f.write('# Test Results Summary\n\n')
              for result in all_results:
                  summary = result['data']['summary']
                  f.write(f\"## {result['file']}\n\n\")
                  f.write(f\"- Total: {summary['total_tests']}\n\")
                  f.write(f\"- Passed: {summary['passed']} ✓\n\")
                  f.write(f\"- Failed: {summary['failed']} ✗\n\")
                  f.write(f\"- Success Rate: {summary['success_rate']}%\n\")
                  f.write(f\"- Duration: {summary['duration_seconds']}s\n\n\")
          "

      - name: Upload combined report
        uses: actions/upload-artifact@v4
        with:
          name: test-report
          path: test-report.md

      - name: Comment PR with results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('test-report.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
