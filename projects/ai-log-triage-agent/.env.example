# AI Log Triage Agent - Environment Configuration
# Copy this file to .env and fill in your values

# ==============================================================================
# LLM API Configuration (REQUIRED)
# ==============================================================================

# Your LLM API key (OpenRouter, OpenAI, etc.)
LLM_API_KEY=your-api-key-here

# API endpoint URL
# Examples:
#   OpenRouter: https://openrouter.ai/api/v1/chat/completions
#   OpenAI: https://api.openai.com/v1/chat/completions
#   Local: http://localhost:11434/v1/chat/completions
LLM_ENDPOINT=https://openrouter.ai/api/v1/chat/completions

# Default model to use
# Examples (OpenRouter):
#   deepseek/deepseek-r1:free
#   x-ai/grok-2-1212:free
#   anthropic/claude-3.5-sonnet
#   openai/gpt-4
# Check your provider's documentation for available models
LLM_DEFAULT_MODEL=your-preferred-model

# ==============================================================================
# LLM Request Configuration (OPTIONAL)
# ==============================================================================

# Request timeout in seconds (default: 120)
# Increase for slower models or networks, decrease for faster response
LLM_TIMEOUT=120

# Maximum tokens in LLM responses (default: 1024)
# Adjust based on your model and use case
LLM_MAX_TOKENS=1024

# ==============================================================================
# Application Configuration (OPTIONAL)
# ==============================================================================

# Directory containing log files (default: ./data)
DATA_DIR=./data
